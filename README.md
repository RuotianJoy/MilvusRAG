# MilvusRAG - 基于向量数据库的智能教育问答系统

## 项目简介

MilvusRAG是一个基于Milvus向量数据库的检索增强生成(RAG)系统，专注于教育领域数据的智能检索与问答。项目集成了学术排名数据（ARWU）和美国高校信息，使用向量相似度搜索实现高效的数据检索和智能问答功能。本项目包含完整的数据处理、向量存储、检索和评估流程，并提供Web界面进行可视化交互。

## 核心特性

- **多维向量表示**：将大学信息转换为评分向量、增强向量和文本向量
- **语义搜索**：基于向量相似度的语义级搜索
- **多语言支持**：中英文双语数据处理和查询
- **自动评测**：基于多种评估指标的RAG系统性能评估
- **精准检索**：结合关键词和向量搜索的混合检索策略
- **灵活配置**：通过配置文件管理系统参数
- **本地嵌入选项**：支持本地和API两种模式生成文本嵌入
- **可视化界面**：Web应用提供文件上传、测试执行、结果分析和数据浏览功能

## 项目架构

```
MilvusRAG/
├── Config/             # 配置文件目录
│   └── Milvus.ini      # Milvus连接配置
├── DataOriginal/       # 原始数据目录
├── DataProcessed/      # 处理后的数据目录
│   ├── ARWU2024_processed.json       # 处理后的ARWU排名数据
│   └── Wiki美国高校初步数据_processed.json  # 处理后的美国高校数据
├── DataProcessing/     # 数据处理脚本
│   ├── ARWU排名数据处理.py           # ARWU排名数据处理
│   └── Wiki美国高校初步数据处理.py    # 维基百科美国高校数据处理
├── DataUploading/      # 数据上传脚本
│   ├── ARWU排名完整导入.py           # ARWU排名数据导入Milvus
│   └── Wiki美国高校初步数据导入.py    # 美国高校数据导入Milvus
├── DataTesting/        # 数据测试目录
├── RAGTesting/         # RAG系统测试
│   ├── RAG_TestingAndEvaluation.py  # 测试与评估脚本
│   ├── auto_test.py                # 自动化测试脚本
│   └── RAG测试问题库及答案.xlsx      # 测试问题和标准答案
├── WebSite/            # Web应用目录
│   ├── app.py                      # Flask应用主文件
│   ├── static/                     # 静态资源(CSS、JS等)
│   ├── templates/                  # HTML模板文件
│   ├── uploads/                    # 上传文件和结果存储
│   └── data/                       # 数据和历史记录
├── TestMilvusConnect.py # Milvus连接测试工具
└── README.md            # 项目文档
```

## 技术栈

- **Python**：主要开发语言
- **Milvus**：向量数据库，用于高效存储和检索向量数据
- **PyMilvus**：Milvus的Python客户端
- **OpenAI API/Deepseek API**：用于生成文本嵌入和LLM推理
- **SentenceTransformers**：本地文本嵌入模型支持
- **ROUGE**：用于评估生成文本质量的指标
- **Pandas/NumPy**：用于数据处理和分析
- **Flask**：Web应用框架
- **Bootstrap**：前端UI框架
- **Chart.js**：数据可视化

## 数据流程

1. **数据处理**：原始数据 → 结构化数据 → 向量化表示
2. **数据导入**：向量化数据 → Milvus集合
3. **检索系统**：用户查询 → 向量化 → 相似度检索 → 相关上下文
4. **生成系统**：检索上下文 + 用户查询 → LLM → 智能回答
5. **效果评测**：使用多种指标评估系统性能
6. **结果展示**：Web界面呈现评测结果和可视化图表

## 评估指标详解

系统使用多种评估指标来全面衡量生成答案的质量，每个指标从不同角度反映了答案的准确性、相关性和完整性。

### ROUGE系列指标

ROUGE（Recall-Oriented Understudy for Gisting Evaluation）是一组广泛应用于文本生成评估的指标。

#### ROUGE-1（词汇重叠度）
- **计算原理**：计算生成答案与参考答案间的单个词语匹配程度
- **数学表达**：
  - 精确率 = 匹配词数量 ÷ 生成答案总词数
  - 召回率 = 匹配词数量 ÷ 参考答案总词数
  - F1 = 2 × 精确率 × 召回率 ÷ (精确率 + 召回率)
- **直观理解**：就像两套词汇卡片的重叠部分，不考虑词语顺序
- **实例说明**：
  - 参考答案："哈佛大学在2024年ARWU排名中位列世界第一"
  - 生成答案："根据ARWU排名，哈佛大学是世界第一的高校"
  - 匹配词："哈佛大学"、"ARWU"、"排名"、"世界"、"第一"
- **应用场景**：评估生成答案是否包含关键词汇
- **解读标准**：分数0.5以上表示词汇覆盖优秀，0.3-0.5表示良好

#### ROUGE-2（双词组匹配度）
- **计算原理**：考虑连续两个词的组合匹配程度，更注重语序
- **数学表达**：与ROUGE-1类似，但统计的是双词组而非单词
- **直观理解**：检测短语结构的保留程度，如"哈佛大学"作为一个完整短语
- **实例说明**：
  - 参考答案："麻省理工学院在工程领域排名第一"
  - 生成答案："在工程领域，麻省理工学院位列榜首"
  - 匹配双词组："麻省理工"、"理工学院"、"工程领域"
- **应用场景**：评估生成答案是否保留了原始句子的短语结构
- **解读标准**：分数通常低于ROUGE-1，0.3以上可视为优秀，0.1-0.3为良好

#### ROUGE-L（最长公共子序列）
- **计算原理**：寻找生成答案与参考答案中最长的相同词语序列（不一定连续）
- **数学基础**：基于最长公共子序列算法（LCS）
- **直观理解**：测量答案的骨架结构相似度，允许中间有不匹配的词
- **实例说明**：
  - 参考答案："斯坦福大学位于加利福尼亚州帕洛阿尔托"
  - 生成答案："斯坦福大学是一所位于加利福尼亚州帕洛阿尔托的研究型大学"
  - 最长公共子序列："斯坦福大学位于加利福尼亚州帕洛阿尔托"
- **应用场景**：评估生成答案是否保留了原文的核心意义和结构
- **解读标准**：分数0.4以上表示结构高度相似，0.2-0.4表示结构基本保留

### 精确率和召回率

这两个指标衡量生成答案的信息准确性和完整性。

#### 精确率（Precision）
- **核心概念**：生成答案中正确信息的比例
- **计算方法**：正确信息点数量 ÷ 生成答案中所有信息点数量
- **实际应用**：通常由人工或AI辅助判断各信息点的正确性
- **优劣判断**：
  - 高精确率：答案几乎不包含错误信息，可靠性高
  - 低精确率：答案含有较多错误或无关信息，可靠性差
- **实例分析**：
  - 生成答案包含10个关于普林斯顿大学的陈述，其中8个正确，精确率为80%
- **应用场景**：特别适合评估需要高准确度的应用，如学术信息检索

#### 召回率（Recall）
- **核心概念**：标准答案中被成功覆盖的信息比例
- **计算方法**：生成答案覆盖的正确信息点 ÷ 标准答案中所有信息点
- **实际应用**：评估答案的完整性和覆盖广度
- **优劣判断**：
  - 高召回率：答案涵盖了大部分应该提到的信息
  - 低召回率：答案遗漏了许多重要信息
- **实例分析**：
  - 标准答案包含耶鲁大学的5个关键事实，生成答案覆盖了其中4个，召回率为80%
- **应用场景**：适合评估需要全面信息的应用，如综合百科问答

### 关键词匹配率

- **定义**：生成答案中包含标准答案关键词的比例
- **计算步骤**：
  1. 从参考答案中提取关键词（如大学名称、排名、专业等）
  2. 检查生成答案中包含这些关键词的数量
  3. 计算匹配率 = 匹配关键词数 ÷ 总关键词数
- **特点**：比ROUGE更聚焦于专有名词和核心术语
- **实例分析**：
  - 参考答案关键词："加州大学伯克利分校"、"计算机科学"、"第四名"、"ARWU排名"
  - 生成答案包含了其中3个关键词，匹配率为75%
- **优势**：
  - 计算简单直观
  - 特别适合评估对专有名词、数字等精确信息的检索能力
  - 不受表达方式变化的影响
- **应用场景**：快速评估答案是否包含必要的专有名词、数字或重要术语

### 多指标协同分析

不同指标的组合可以提供更全面的评估视角：

- **精确率高但召回率低**：生成答案简短准确，但不全面
- **召回率高但精确率低**：生成答案冗长，包含了过多无关或错误信息
- **ROUGE-1高但ROUGE-2/L低**：答案包含正确词汇但语句结构差异大
- **关键词匹配率高但ROUGE分数低**：抓住了核心名词术语，但表达方式完全不同

### 评分解读

- **0.5以上**：
  - 信息高度准确且全面
  - 语言表达接近标准答案
  - 几乎包含所有关键词和核心信息点
- **0.3-0.5**：
  - 信息基本准确，可能有小遗漏
  - 表达方式与标准答案有一定差异
  - 包含大部分关键词和核心观点
- **0.1-0.3**：
  - 信息部分准确，有明显遗漏或错误
  - 表达方式与标准答案差异较大
  - 关键词覆盖率中等
- **0.1以下**：
  - 信息严重不足或有大量错误
  - 检索或生成环节可能存在明显问题
  - 关键词覆盖率低

## 快速开始

### 环境要求

- Python 3.10+
- Milvus 2.3.0+
- OpenAI API/Deepseek API访问权限

### 安装与配置

1. **安装Milvus**

   参考[Milvus官方文档](https://milvus.io/docs)安装Milvus服务

   ```bash
   # 使用Docker安装Milvus (推荐)
   docker run -d --name milvus_standalone -p 19530:19530 -p 9091:9091 milvus/milvus:latest standalone
   ```

2. **安装环境依赖包**

   ```bash
   pip install -r requirements.txt
   ```

3. **配置Milvus连接**

   修改`Config/Milvus.ini`配置文件中的连接参数

4. **测试Milvus连接**

   ```bash
   python TestMilvusConnect.py
   ```

### 数据处理与导入

1. **处理ARWU排名数据**

   ```bash
   python DataProcessing/ARWU排名数据处理.py
   ```

2. **将处理后的数据导入Milvus**

   ```bash
   python DataUploading/ARWU排名完整导入.py
   ```


### 命令行测试与评估

使用评测脚本测试RAG系统性能：

```bash
python RAGTesting/RAG_TestingAndEvaluation.py
```

### 启动Web应用

1. **安装Web应用依赖**

   ```bash
   cd WebSite
   pip install -r requirements.txt
   ```

2. **运行Flask应用**

   ```bash
   python app.py
   ```

   应用将在 `http://localhost:5555` 上启动。

## Web应用功能

### 1. 测试文件上传
- **功能说明**：上传包含测试问题和标准答案的Excel或CSV文件
- **文件要求**：必须包含`Questions`和`Answers`两列
- **操作方式**：点击"选择文件"按钮，然后选择符合要求的测试文件

### 2. 执行测试
- **功能说明**：系统将使用RAG流程处理每个测试问题
- **处理流程**：
  1. 将问题转换为向量
  2. 在Milvus中搜索相关知识
  3. 使用大语言模型生成回答
  4. 对比生成答案和标准答案，计算评分
- **实时反馈**：测试过程中会显示进度和中间结果

### 3. 结果分析
- **详细报告**：每个问题的评分和统计数据
- **可视化图表**：各项指标的平均分和分布情况
- **导出功能**：支持下载完整测试结果为CSV格式

### 4. 数据浏览
- **集合概览**：显示Milvus中的所有数据集合
- **数据结构**：展示每个集合的字段和属性
- **数据样本**：查看每个集合的示例数据
- **统计信息**：数据量、索引类型等核心信息

## 注意事项

- Web应用默认只处理前50个测试问题，以避免长时间运行
- 确保Milvus服务正常运行且已导入数据
- 测试结果会保存在`WebSite/uploads`目录下的CSV文件中
- 在使用前请确保配置了正确的API密钥
- 如需处理大量测试问题，建议使用命令行版本的测试脚本


